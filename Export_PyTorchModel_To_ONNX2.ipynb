{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LW7qEZkwRGu-",
        "outputId": "372f7aa8-946f-43c7-9630-8174f477c0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx"
      ],
      "metadata": {
        "id": "EzECtODrRssc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class SuperResolutionNet(nn.Module):\n",
        "    def __init__(self, upscale_factor):\n",
        "        super(SuperResolutionNet, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pixel_shuffle(self.conv4(x))\n",
        "        return x\n",
        "\n",
        "torch_model = SuperResolutionNet(upscale_factor=3)"
      ],
      "metadata": {
        "id": "wW0RpaS7RVnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
        "batch_size = 64    # just a random number\n",
        "\n",
        "# Initialize model with the pretrained weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
        "\n",
        "# set the model to inference mode\n",
        "torch_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbrizqLERik4",
        "outputId": "e6e68518-6922-431c-f494-aa2ba2792f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\" to /root/.cache/torch/hub/checkpoints/superres_epoch100-44c6958e.pth\n",
            "100%|██████████| 234k/234k [00:00<00:00, 1.31MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuperResolutionNet(\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
        "torch_out = torch_model(x)\n",
        "\n",
        "torch.onnx.export(torch_model,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})"
      ],
      "metadata": {
        "id": "m-HUJZGmR1Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "FP4SbbwcR6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "# compare ONNX Runtime and PyTorch results\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0f_uydRSCTe",
        "outputId": "2a7d0491-1d2b-4678-a307-1fd2713b875d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
        "\n",
        "start = time.time()\n",
        "torch_out = torch_model(x)\n",
        "end = time.time()\n",
        "print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
        "\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "start = time.time()\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "end = time.time()\n",
        "print(f\"Inference of ONNX model used {end - start} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE5D0rOVSIWn",
        "outputId": "ba549961-5a11-491a-ac98-fdfda72a0696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference of Pytorch model used 8.932272672653198 seconds\n",
            "Inference of ONNX model used 4.420607089996338 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "img = Image.open(\"/content/10.jpg\")\n",
        "\n",
        "resize = transforms.Resize([224, 224])\n",
        "img = resize(img)\n",
        "\n",
        "img_ycbcr = img.convert('YCbCr')\n",
        "img_y, img_cb, img_cr = img_ycbcr.split()\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "img_y = to_tensor(img_y)\n",
        "img_y.unsqueeze_(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExdYvy7ySLun",
        "outputId": "f076ebc5-c519-4c18-fa9c-819afc09a97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.2157, 0.1961, 0.1922,  ..., 0.5294, 0.5569, 0.5686],\n",
              "          [0.2039, 0.1961, 0.1922,  ..., 0.5333, 0.5569, 0.5686],\n",
              "          [0.1961, 0.1843, 0.1843,  ..., 0.5216, 0.5412, 0.5490],\n",
              "          ...,\n",
              "          [0.6667, 0.6745, 0.6392,  ..., 0.6902, 0.6667, 0.6078],\n",
              "          [0.6392, 0.6431, 0.6235,  ..., 0.8000, 0.7608, 0.6745],\n",
              "          [0.6392, 0.6353, 0.6510,  ..., 0.8118, 0.7686, 0.6667]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "img_out_y = ort_outs[0]"
      ],
      "metadata": {
        "id": "BYKBMDJlSUTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(img_out_y, Image.Image):\n",
        "    img_out_y = np.array(img_out_y)\n",
        "\n",
        "img_out_y = Image.fromarray(np.uint8((img_out_y * 255.0).clip(0, 255)), mode='L')\n",
        "# get the output image follow post-processing step from PyTorch implementation\n",
        "final_img = Image.merge(\n",
        "    \"YCbCr\", [\n",
        "        img_out_y,\n",
        "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
        "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
        "    ]).convert(\"RGB\")\n",
        "\n",
        "# Save the image, we will compare this with the output image from mobile device\n",
        "final_img.save(\"/content/10_with_ort.jpg\")\n",
        "\n",
        "# Save resized original image (without super-resolution)\n",
        "img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img)\n",
        "img.save(\"cat_resized.jpg\")"
      ],
      "metadata": {
        "id": "k9U3-0FFSWrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsr7GW7TSzkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}